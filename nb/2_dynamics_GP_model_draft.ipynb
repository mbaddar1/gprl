{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "1. develop kernel \n",
    "2. validate kernel with h-param variation as in ref#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import pearsonr\n",
    "from numpy.linalg import cholesky, det, lstsq,inv\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(xp,xq,p,q,lambda_diag,v,sigma_n):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(level=logging.DEBUG)\n",
    "    \"\"\"\n",
    "    xp: float array mx1 vector of input point p\n",
    "    xq : float array\n",
    "    lambda_ 1Xm represents the diagonal of the hyperparemter diagonal matrix lambda_\n",
    "    v: hyperparemter\n",
    "    sigma2_n : hyperparameter\n",
    "    \"\"\"\n",
    "    #TODO check all inputes are doubles\n",
    "    lambda_power_minus_1 = np.diag(np.reciprocal(lambda_diag))\n",
    "    exponent_ = -0.5*np.matmul(np.matmul((xp-xq).T,lambda_power_minus_1),(xp-xq))\n",
    "    delta_pq = 1.0 if p==q else 0.0\n",
    "    kernel = v**2 * np.exp(exponent)+delta_pq*sigma_n**2\n",
    "    #logging\n",
    "    logger.debug(f'lambda diag= {lambda_diag}')\n",
    "    logger.debug(f'lambda_pow_minus_1 {lambda_power_minus_1}')\n",
    "    logger.debug(f'term1 = -(xp-xq).T * lambda_pow_minus_1 = {term1}')\n",
    "    logger.debug(f'exponent = {exponent_}')\n",
    "    ######\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't delete\n",
    "xp = pd.Series([1.0,2.0]).values.reshape(-1,1)\n",
    "xq = pd.Series([1.0,2.0]).values.reshape(-1,1)\n",
    "lambda_diag = [0.2,0.3]\n",
    "lambda_power_minus_1 = np.diag(np.reciprocal(lambda_diag))\n",
    "lambda_power_minus_1\n",
    "(xp-xq).T.shape\n",
    "term1 = np.matmul(-(xp-xq).T,lambda_power_minus_1)\n",
    "exponent = np.matmul(term1,(xp-xq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:lambda diag= [0.2, 0.3]\n",
      "DEBUG:root:lambda_pow_minus_1 [[5.         0.        ]\n",
      " [0.         3.33333333]]\n",
      "DEBUG:root:term1 = -(xp-xq).T * lambda_pow_minus_1 = [[0. 0.]]\n",
      "DEBUG:root:exponent = [[-0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k= kernel(xp=xp,xq=xq,p=1,q=2,lambda_diag=lambda_diag,sigma_n=0.05,v=1.0)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# quick validation :kernel vs euclidean distance , comment out , time consuming\n",
    "# dim = 100,5\n",
    "# X = np.random.normal(loc=0,scale=1.0,size=dim)\n",
    "# k = np.zeros(dim[0]**2)\n",
    "# euc = np.zeros(dim[0]**2)\n",
    "# idx = 0\n",
    "# for i in np.arange(dim[0]):\n",
    "#     for j in np.arange(dim[0]):\n",
    "#         xp = X[i,].reshape(-1,1)\n",
    "#         xq = X[j,].reshape(-1,1)\n",
    "#         k[idx] = kernel(xp=xp,xq=xq,p=i,q=j,lambda_diag=[0.2,0.1,0.3,0.4,0.5],sigma_n=0.05,v=1.0)\n",
    "#         euc[idx] = euclidean(u=xp,v=xq)\n",
    "#         idx = idx + 1\n",
    "# pearsonr(x=k,y=np.reciprocal(euc+0.01))\n",
    "# if corr is +ve and significant, then we are good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test scipy minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "def fn(x,x0,c):\n",
    "    return np.sum((x-x0)**2)+c\n",
    "x0 = np.array([[1,1]])\n",
    "x = np.array([[2,2]])\n",
    "\n",
    "x_min = minimize(fun=fn,x0=np.array([[0,0]]),args=(x0,10))\n",
    "x_min.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def kernel as matrix operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Note: in http://krasserm.github.io/2018/03/19/gaussian-processes/\n",
    "# in the implementatoin of Gaussian kernel , the sum of the two squared terms is broadcast sum\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.add.html\n",
    "# https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\n",
    "# why ? as in first term , reshape(-1,1) is used , then the row sum shape is converted from (m,) to (m,1)\n",
    "# then it is summed with array of shape (n,) then shapes are not equal, even if m==n , then the second dimensions \n",
    "# are not equal\n",
    "import numpy as np\n",
    "def kernel(X1, X2, l=1.0, sigma_f=1.0):\n",
    "    ''' Isotropic squared exponential kernel. Computes a covariance matrix from points in X1 and X2. Args: X1: Array of m points (m x d). X2: Array of n points (n x d). Returns: Covariance matrix (m x n). '''\n",
    "    # the sume is broad cast \n",
    "    sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist)\n",
    "\n",
    "X1 = np.random.normal(loc = 10,scale = 2, size = (10,2))\n",
    "X2 = np.random.normal(loc = 10,scale = 2, size = (10,2))\n",
    "X1_sum= np.sum(X1**2, 1).reshape(-1,1)\n",
    "X2_sum= np.sum(X2**2, 1)\n",
    "X2_sum_2= np.sum(X2**2, 1).reshape(-1,1)\n",
    "tot = X1_sum + X2_sum\n",
    "print((X1_sum + X2_sum).shape)\n",
    "print((X1_sum + X2_sum_2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0025, 1.0025, 1.0025, 1.0025, 1.0025, 1.0025, 1.0025, 1.0025,\n",
       "       1.0025, 1.0025])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def kernel_mtx(X1,X2,lambda_diag,v,sigma_n):\n",
    "    ## from http://krasserm.github.io/2018/03/19/gaussian-processes/\n",
    "    ## sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(level=logging.INFO)\n",
    "    assert X1.shape[1] == X2.shape[1]\n",
    "    assert X1.shape[1] == len(lambda_diag)\n",
    "    \n",
    "    \n",
    "    lambda_power_minus_1 = np.diag(np.reciprocal(lambda_diag))\n",
    "    # quadratic terms are scale by inverse precision (variance) , precision = lambda (Captial case)\n",
    "    X1_sq_sum_scaled = np.sum(a=np.matmul(X1**2,lambda_power_minus_1),axis=1)\n",
    "    X2_sq_sum_scale = np.sum(a=np.matmul(X2**2,lambda_power_minus_1),axis=1)\n",
    "    X1_matmul_X2_T_scaled = np.matmul(np.matmul(X1,lambda_power_minus_1),X2.T)\n",
    "    \n",
    "    sqdist_scaled = X1_sq_sum_scaled.reshape(-1,1)+X2_sq_sum_scale-2*X1_matmul_X2_T_scaled\n",
    "    sigma_n_sq = np.zeros((X1.shape[0],X2.shape[0]))\n",
    "    if X1.shape[0] == X2.shape[0]: #same data-set , or at least comparable\n",
    "        sigma_n_sq = np.diag(np.repeat(sigma_n**2,X1.shape[0]))\n",
    "        \n",
    "    kernel = v**2 * np.exp(-0.5*sqdist_scaled)+sigma_n_sq\n",
    "    logger.debug(f'sqdist_scaled = {sqdist_scaled}')\n",
    "    return kernel\n",
    "\n",
    "\n",
    "M = 10\n",
    "N = 8\n",
    "D = 3\n",
    "X1 = np.random.normal(loc = 10, scale  = 2, size = (M,D))\n",
    "X2 = np.random.normal(loc = 5, scale  = 1, size = (N,D))\n",
    "lambda_diag = np.random.normal(loc = 1, scale = 0.01, size = D)\n",
    "v = 1\n",
    "sigma_n = 0.05\n",
    "X1.shape\n",
    "k = kernel_mtx(X1,X1,lambda_diag,v,sigma_n)\n",
    "np.diag(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packing theta\n",
    "def kernel_fn(theta,X1,X2):\n",
    "    assert X1.shape[1] == X2.shape[1]\n",
    "    D = X1.shape[1]\n",
    "    lambda_diag=theta[0:D]\n",
    "    v= theta[D]\n",
    "    sigma_n = theta[D+1]\n",
    "    return kernel_mtx(X1,X1,lambda_diag,v,sigma_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.3733470976922"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y1 = np.random.normal(loc=2,scale = 1,size=(M,2))\n",
    "theta_0 = np.random.normal(loc=2,scale = 1,size=X1.shape[1]+2)\n",
    "def neg_ll(theta,X,t,kernel_fn):\n",
    "    assert X.shape[0]==t.shape[0]\n",
    "    K = kernel_fn(theta,X,X)\n",
    "    #Y_train.T.dot(inv(K).dot(Y_train)\n",
    "    neg_ll_tot = 0\n",
    "    for j in np.arange(t.shape[1]):    \n",
    "        neg_ll_j = np.log(det(K))+t[:,j].T.dot(inv(K).dot(t[:,j]))\n",
    "        neg_ll_tot = neg_ll_tot+neg_ll_j\n",
    "    return neg_ll_tot\n",
    "t = np.random.normal(loc=5,scale = 1,size = (M,1))\n",
    "neg_ll(X=X1,t=t,kernel_fn=kernel_fn,theta=theta_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 15.21885460868222\n",
       " hess_inv: array([[ 5.52464269,  1.55265189,  4.08385568, -3.40913393,  0.0895512 ],\n",
       "       [ 1.55265197,  2.78212314,  2.45071527,  1.12476522,  0.01832404],\n",
       "       [ 4.08385563,  2.45071538,  5.56792602, -1.12412138,  0.08199106],\n",
       "       [-3.40913393,  1.12476522, -1.12412138,  6.83022595, -0.07083123],\n",
       "       [ 0.0895512 ,  0.01832404,  0.08199107, -0.07083123,  0.03768105]])\n",
       "      jac: array([-1.54972076e-06, -3.57627869e-07, -8.34465027e-07, -5.12599945e-06,\n",
       "       -4.17232513e-06])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 1033\n",
       "      nit: 60\n",
       "     njev: 147\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([ 4.74424872e+04,  3.34621475e+04,  5.52201513e+04,  5.30698979e+00,\n",
       "       -9.72720093e-01])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(fun=neg_ll,x0=theta_0,args=(X1,t,kernel_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -530.1244889056069\n",
       " hess_inv: <5x5 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([ 0.02240768, -0.18090986, -0.31222953,  0.01585931,  6.23232381])\n",
       "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "     nfev: 540\n",
       "      nit: 38\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([ 1.27182150e+02,  8.74043931e+01,  8.49225624e+01,  3.49725494e+00,\n",
       "       -1.40475178e-02])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('../data/train_data.csv')\n",
    "train_data.head()\n",
    "X = train_data[['x','x_dot','F']]\n",
    "D_x = X.values.shape[1]\n",
    "theta_0 = np.random.normal(loc=2,scale = 1,size=D_x+2)\n",
    "t = train_data[['x_prime','x_dot_prime']]\n",
    "minimize(fun=neg_ll,x0=theta_0,args=(X.values,t.values,kernel_fn),method='L-BFGS-B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get the error <br>\n",
    "\"\"\"\n",
    "fun: -451.7428515599334\n",
    " hess_inv: array([[ 1.31958831e+01,  2.25244206e+01,  6.35074363e+01,\n",
    "        -3.21234371e-01, -2.18691817e-03],\n",
    "       [ 2.25244206e+01,  3.85101858e+01,  1.08902122e+02,\n",
    "        -5.40652637e-01, -3.74693823e-03],\n",
    "       [ 6.35074363e+01,  1.08902122e+02,  3.10097706e+02,\n",
    "        -1.51666971e+00, -1.06503453e-02],\n",
    "       [-3.21234371e-01, -5.40652637e-01, -1.51666971e+00,\n",
    "         1.09053674e-02,  5.24254177e-05],\n",
    "       [-2.18691817e-03, -3.74693823e-03, -1.06503453e-02,\n",
    "         5.24254177e-05,  3.73928680e-07]])\n",
    "      jac: array([ 2.96734238e+00, -6.96512985e+00, -5.29705048e-01,  4.49545670e+00,\n",
    "       -4.81347301e+04])\n",
    "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
    "     nfev: 877\n",
    "      nit: 20\n",
    "     njev: 124\n",
    "   status: 2\n",
    "  success: False\n",
    "        x: array([ 4.25329147e+01,  5.54329651e+01,  1.48233823e+02, -2.11552736e+00,\n",
    "       -2.27977169e-04])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_2(X1, X2, l=1.0, sigma_f=1.0):\n",
    "    ''' Isotropic squared exponential kernel. Computes a covariance matrix from points in X1 and X2. Args: X1: Array of m points (m x d). X2: Array of n points (n x d). Returns: Covariance matrix (m x n). '''\n",
    "    sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist)\n",
    "\n",
    "def nll_fn_2(X_train, Y_train, noise, naive=True):\n",
    "    ''' Returns a function that computes the negative marginal log- likelihood for training data X_train and Y_train and given noise level. Args: X_train: training locations (m x d). Y_train: training targets (m x 1). noise: known noise level of Y_train. naive: if True use a naive implementation of Eq. (7), if False use a numerically more stable implementation. Returns: Minimization objective. '''\n",
    "    def nll_naive(theta):\n",
    "        # Naive implementation of Eq. (7). Works well for the examples \n",
    "        # in this article but is numerically less stable compared to \n",
    "        # the implementation in nll_stable below.\n",
    "        K = kernel_2(X_train, X_train, l=theta[0], sigma_f=theta[1]) + \\\n",
    "            noise**2 * np.eye(len(X_train))\n",
    "        nll_tot = 0\n",
    "        for j in np.arange(Y_train.shape[1]):\n",
    "            nll_j=0.5 * np.log(det(K)) + \\\n",
    "                        0.5 * Y_train[:,j].T.dot(inv(K).dot(Y_train[:,j])) + \\\n",
    "                        0.5 * len(X_train) * np.log(2*np.pi)\n",
    "            nll_tot = nll_tot+nll_j\n",
    "            return nll_tot\n",
    "\n",
    "    def nll_stable_2(theta):\n",
    "        # Numerically more stable implementation of Eq. (7) as described\n",
    "        # in http://www.gaussianprocess.org/gpml/chapters/RW2.pdf, Section\n",
    "        # 2.2, Algorithm 2.1.\n",
    "        K = kernel_2(X_train, X_train, l=theta[0], sigma_f=theta[1]) + \\\n",
    "            noise**2 * np.eye(len(X_train))\n",
    "        L = cholesky(K)\n",
    "        return np.sum(np.log(np.diagonal(L))) + \\\n",
    "               0.5 * Y_train.T.dot(lstsq(L.T, lstsq(L, Y_train)[0])[0]) + \\\n",
    "               0.5 * len(X_train) * np.log(2*np.pi)\n",
    "    \n",
    "    if naive:\n",
    "        return nll_naive\n",
    "    else:\n",
    "        return nll_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 16.363283882242314\n",
       " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([-0.00014388, -0.00010907])\n",
       "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "     nfev: 105\n",
       "      nit: 23\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([36.53374974, 16.2735757 ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise = 0.4\n",
    "#k = kernel_2(X1=X.values,X2=X.values)\n",
    "nll_fn_2_ = nll_fn_2(X_train=X.values,Y_train=t.values,noise=noise)\n",
    "nll_fn_2_([1,1])\n",
    "res = minimize(nll_fn_2_, [1, 1], \n",
    "               bounds=((1e-5, None), (1e-5, None)),\n",
    "               method='L-BFGS-B')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to verify , plot predictive distribution for values from kernel 1 and kernel 2 , should be the same !\n",
    "# is it called similarity verification !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ref:\n",
    "1. http://krasserm.github.io/2018/03/19/gaussian-processes/\n",
    "2. https://papers.nips.cc/paper/2420-gaussian-processes-in-reinforcement-learning.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
