{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import pearsonr\n",
    "from numpy.linalg import cholesky, det, lstsq,inv\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref [2] eq(5)\n",
    "def kernel_vec_gaussian_w_precision(xp,xq,p,q,lambda_diag,v,sigma_n):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(level=logging.DEBUG)\n",
    "    \"\"\"\n",
    "    xp: float array mx1 vector of input point p\n",
    "    xq : float array\n",
    "    lambda_ 1Xm represents the diagonal of the hyperparemter diagonal matrix lambda_\n",
    "    v: hyperparemter\n",
    "    sigma2_n : hyperparameter\n",
    "    \"\"\"\n",
    "    #TODO check all inputes are doubles\n",
    "    lambda_power_minus_1 = np.diag(np.reciprocal(lambda_diag))\n",
    "    exponent_ = -0.5*np.matmul(np.matmul((xp-xq).T,lambda_power_minus_1),(xp-xq))\n",
    "    delta_pq = 1.0 if p==q else 0.0\n",
    "    kernel = v**2 * np.exp(exponent)+delta_pq*sigma_n**2\n",
    "    #logging\n",
    "    logger.debug(f'lambda diag= {lambda_diag}')\n",
    "    logger.debug(f'lambda_pow_minus_1 {lambda_power_minus_1}')\n",
    "    logger.debug(f'term1 = -(xp-xq).T * lambda_pow_minus_1 = {term1}')\n",
    "    logger.debug(f'exponent = {exponent_}')\n",
    "    ######\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# quick validation :kernel vs euclidean distance , comment out , time consuming\n",
    "# dim = 100,5\n",
    "# X = np.random.normal(loc=0,scale=1.0,size=dim)\n",
    "# k = np.zeros(dim[0]**2)\n",
    "# euc = np.zeros(dim[0]**2)\n",
    "# idx = 0\n",
    "# for i in np.arange(dim[0]):\n",
    "#     for j in np.arange(dim[0]):\n",
    "#         xp = X[i,].reshape(-1,1)\n",
    "#         xq = X[j,].reshape(-1,1)\n",
    "#         k[idx] = kernel(xp=xp,xq=xq,p=i,q=j,lambda_diag=[0.2,0.1,0.3,0.4,0.5],sigma_n=0.05,v=1.0)\n",
    "#         euc[idx] = euclidean(u=xp,v=xq)\n",
    "#         idx = idx + 1\n",
    "# pearsonr(x=k,y=np.reciprocal(euc+0.01))\n",
    "# if corr is +ve and significant, then we are good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Note: in http://krasserm.github.io/2018/03/19/gaussian-processes/\n",
    "# in the implementatoin of Gaussian kernel , the sum of the two squared terms is broadcast sum\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.add.html\n",
    "# https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\n",
    "# why ? as in first term , reshape(-1,1) is used , then the row sum shape is converted from (m,) to (m,1)\n",
    "# then it is summed with array of shape (n,) then shapes are not equal, even if m==n , then the second dimensions \n",
    "# are not equal\n",
    "import numpy as np\n",
    "def kernel_gaussian(X1, X2, l=1.0, sigma_f=1.0):\n",
    "    ''' Isotropic squared exponential kernel. Computes a covariance matrix from points in X1 and X2. Args: X1: Array of m points (m x d). X2: Array of n points (n x d). Returns: Covariance matrix (m x n). '''\n",
    "    # the sume is broad cast \n",
    "    sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist)\n",
    "\n",
    "X1 = np.random.normal(loc = 10,scale = 2, size = (10,2))\n",
    "X2 = np.random.normal(loc = 10,scale = 2, size = (10,2))\n",
    "X1_sum= np.sum(X1**2, 1).reshape(-1,1)\n",
    "X2_sum= np.sum(X2**2, 1)\n",
    "X2_sum_2= np.sum(X2**2, 1).reshape(-1,1)\n",
    "tot = X1_sum + X2_sum\n",
    "print((X1_sum + X2_sum).shape)\n",
    "print((X1_sum + X2_sum_2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0025, 1.0025, 1.0025, 1.0025, 1.0025, 1.0025, 1.0025, 1.0025,\n",
       "       1.0025, 1.0025])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ref [2] eq(5)\n",
    "def kernel_mtx_gaussian_w_precision(X1,X2,lambda_diag,v,sigma_n):\n",
    "    ## from http://krasserm.github.io/2018/03/19/gaussian-processes/\n",
    "    ## sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(level=logging.INFO)\n",
    "    assert X1.shape[1] == X2.shape[1]\n",
    "    assert X1.shape[1] == len(lambda_diag)\n",
    "    \n",
    "    \n",
    "    lambda_power_minus_1 = np.diag(np.reciprocal(lambda_diag))\n",
    "    # quadratic terms are scale by inverse precision (variance) , precision = lambda (Captial case)\n",
    "    X1_sq_sum_scaled = np.sum(a=np.matmul(X1**2,lambda_power_minus_1),axis=1)\n",
    "    X2_sq_sum_scaled = np.sum(a=np.matmul(X2**2,lambda_power_minus_1),axis=1)\n",
    "    X1_matmul_X2_T_scaled = np.matmul(np.matmul(X1,lambda_power_minus_1),X2.T)\n",
    "    \n",
    "    sqdist_scaled = X1_sq_sum_scaled.reshape(-1,1)+X2_sq_sum_scaled-2*X1_matmul_X2_T_scaled\n",
    "    sigma_n_sq = np.zeros((X1.shape[0],X2.shape[0]))\n",
    "    if X1.shape[0] == X2.shape[0]: #same data-set , or at least comparable\n",
    "        sigma_n_sq = np.diag(np.repeat(sigma_n**2,X1.shape[0]))\n",
    "        \n",
    "    kernel = v**2 * np.exp(-0.5*sqdist_scaled)+sigma_n_sq\n",
    "    logger.debug(f'sqdist_scaled = {sqdist_scaled}')\n",
    "    return kernel\n",
    "\n",
    "\n",
    "M = 10\n",
    "N = 8\n",
    "D = 3\n",
    "X1 = np.random.normal(loc = 10, scale  = 2, size = (M,D))\n",
    "X2 = np.random.normal(loc = 5, scale  = 1, size = (N,D))\n",
    "lambda_diag = np.random.normal(loc = 1, scale = 0.01, size = D)\n",
    "v = 1\n",
    "sigma_n = 0.05\n",
    "\n",
    "k = kernel_mtx_gaussian_w_precision(X1,X1,lambda_diag,v,sigma_n)\n",
    "np.diag(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packing theta\n",
    "def kernel_mtx_gaussian_w_precision_fn(theta,X1,X2):\n",
    "    assert X1.shape[1] == X2.shape[1]\n",
    "    D = X1.shape[1]\n",
    "    lambda_diag=theta[0:D]\n",
    "    v= theta[D]\n",
    "    sigma_n = theta[D+1]\n",
    "    return kernel_mtx_gaussian_w_precision(X1,X2,lambda_diag,v,sigma_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_gaussian_fn(theta,X1,X2):\n",
    "    assert X1.shape[1] == X2.shape[1]\n",
    "    l = theta[0]\n",
    "    sigma_f = theta[1]\n",
    "    return kernel_gaussian(X1, X2, l, sigma_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.20351007416716"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calc neg_LL (ignore constant terms , deosn't affect optimization results)\n",
    "def neg_ll(theta,X,t,kernel_fn):\n",
    "    assert X.shape[0]==t.shape[0]\n",
    "    K = kernel_fn(theta,X,X)\n",
    "    neg_ll_tot = 0\n",
    "    for j in np.arange(t.shape[1]):    \n",
    "        neg_ll_j = np.log(det(K))+t[:,j].T.dot(inv(K).dot(t[:,j]))\n",
    "        neg_ll_tot = neg_ll_tot+neg_ll_j\n",
    "    return neg_ll_tot\n",
    "\n",
    "t = np.random.normal(loc=5,scale = 1,size = (M,1))\n",
    "theta_0 = np.random.normal(loc=2,scale = 1,size=X1.shape[1]+2)\n",
    "neg_ll(X=X1,t=t,kernel_fn=kernel_mtx_gaussian_w_precision_fn,theta=theta_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 15.35723797945462\n",
       " hess_inv: array([[ 3.07453542e-01,  1.80094333e+00,  1.27527574e+00,\n",
       "         1.16019856e+00,  2.79225127e-02],\n",
       "       [ 1.80094336e+00,  4.12654526e+01,  1.52867817e+01,\n",
       "         3.04558647e+00, -9.67237500e-02],\n",
       "       [ 1.27527576e+00,  1.52867817e+01,  7.33060558e+00,\n",
       "         3.92081993e+00,  6.45276123e-02],\n",
       "       [ 1.16019856e+00,  3.04558647e+00,  3.92081993e+00,\n",
       "         4.91591508e+00,  1.58584747e-01],\n",
       "       [ 2.79225127e-02, -9.67237500e-02,  6.45276123e-02,\n",
       "         1.58584747e-01,  2.28735542e-02]])\n",
       "      jac: array([-1.12056732e-05, -8.10623169e-06,  1.00135803e-05,  7.67707825e-05,\n",
       "       -1.08146667e-03])\n",
       "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
       "     nfev: 1354\n",
       "      nit: 74\n",
       "     njev: 192\n",
       "   status: 2\n",
       "  success: False\n",
       "        x: array([ 8.40068101e+03,  1.57480286e+03,  7.07204941e+03,  5.67060044e+00,\n",
       "       -9.20966258e-01])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first trial for minimization\n",
    "minimize(fun=neg_ll,x0=theta_0,args=(X1,t,kernel_mtx_gaussian_w_precision_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note : \n",
    "With default method (not show or known yet) , optimization doesn't converge. Try with explcitly defined methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 15.426999965247807\n",
       " hess_inv: <5x5 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([-2.20268248e-05,  2.82440737e-05,  1.33226763e-05, -7.40740802e-05,\n",
       "        1.75823800e-03])\n",
       "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "     nfev: 516\n",
       "      nit: 55\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([ 5.18687352e+03,  3.53176184e+03,  4.63838946e+03,  5.64213874e+00,\n",
       "       -9.29346919e-01])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(fun=neg_ll,x0=theta_0,args=(X1,t,kernel_mtx_gaussian_w_precision_fn),method=\"L-BFGS-B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "with L-BFGS-B , the optimization converges and success. Hence, see TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustrative example in [2] Section 3, Dynamics GP Model Identification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('../data/train_data2019-10-01T09:12:32.226141.csv')\n",
    "train_data.head()\n",
    "X = train_data[['x','x_dot','F']].values\n",
    "D_x = X.shape[1]\n",
    "t = train_data[['x_prime','x_dot_prime']].values\n",
    "t_prime = t-np.mean(t,axis=0) # to satisfy E(t) = 0 condition , used for ll calculations\n",
    "N = X.shape[0]\n",
    "N_t = 50 # int(np.floor(0.8*X.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:minimize results at train index = 0 = \n",
      "      fun: -502.84640527423\n",
      " hess_inv: <5x5 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([-0.17197976, -0.06907044, -0.12272494,  0.0413138 , -0.06933192])\n",
      "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 714\n",
      "      nit: 40\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 3.80352829e+01,  4.67392363e+01,  7.75338865e+01, -3.09417322e+00,\n",
      "       -1.28534036e-02])\n",
      "INFO:root:minimize results at train index = 1 = \n",
      "      fun: -523.3607130580577\n",
      " hess_inv: <5x5 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([-0.15122623, -0.22690756, -0.28122713, -0.02656861,  0.14352963])\n",
      "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 642\n",
      "      nit: 42\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([5.43041841e+01, 8.05018718e+01, 1.58681050e+02, 3.49093729e+00,\n",
      "       1.52129489e-02])\n",
      "/home/mbaddar/mbaddar/my_projects/phd/implementation/gprl/gprl_venv/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in log\n",
      "  import sys\n",
      "/home/mbaddar/mbaddar/my_projects/phd/implementation/gprl/gprl_venv/lib/python3.7/site-packages/numpy/linalg/linalg.py:2125: RuntimeWarning: overflow encountered in det\n",
      "  r = _umath_linalg.det(a, signature=signature)\n",
      "/home/mbaddar/mbaddar/my_projects/phd/implementation/gprl/gprl_venv/lib/python3.7/site-packages/scipy/optimize/optimize.py:696: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  grad[k] = (f(*((xk + d,) + args)) - f0) / d[k]\n",
      "INFO:root:minimize results at train index = 2 = \n",
      "      fun: 1242.1294596025855\n",
      " hess_inv: <5x5 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([-4.54747351e-05, -3.18323146e-04,  0.00000000e+00,  2.00088834e-03,\n",
      "        4.39240466e-01])\n",
      "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 294\n",
      "      nit: 26\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([-6.74178498e+04, -6.97648626e+04, -1.23603974e+06, -2.49625326e+04,\n",
      "        4.40605596e+02])\n",
      "INFO:root:minimize results at train index = 3 = \n",
      "      fun: -129.1339297394656\n",
      " hess_inv: <5x5 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([ -9.30128579,  -8.35779872, -16.57853659,   5.40470637,\n",
      "       441.85876504])\n",
      "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 72\n",
      "      nit: 5\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 2.40146865,  2.9324825 ,  3.20608611, -0.85784416,  0.11234295])\n",
      "INFO:root:minimize results at train index = 4 = \n",
      "      fun: -435.8260731757638\n",
      " hess_inv: <5x5 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([ 1.91745357e+00,  6.65457947e+00, -3.64298103e-01, -1.18636763e+00,\n",
      "        8.32177182e+02])\n",
      "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 342\n",
      "      nit: 20\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 1.31048695e+02,  9.27044121e+01,  1.30196274e+02, -2.85351416e+01,\n",
      "        1.20357319e-02])\n",
      "INFO:root:minimize results at train index = 5 = \n",
      "      fun: -505.37285537420297\n",
      " hess_inv: <5x5 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([-0.0860723 , -0.42819011, -0.19355753,  0.16015065, -9.08196398])\n",
      "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 612\n",
      "      nit: 35\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 5.04213294e+01,  3.79156280e+01,  8.96706932e+01, -3.09875134e+00,\n",
      "       -1.35179296e-02])\n",
      "INFO:root:minimize results at train index = 6 = \n",
      "      fun: -534.439503236762\n",
      " hess_inv: <5x5 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([-0.07851213, -0.1561375 , -0.20070274,  0.33303422, -0.0634941 ])\n",
      "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 996\n",
      "      nit: 66\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 1.23107012e+02,  1.85601186e+02,  1.83846655e+02,  4.58863106e+00,\n",
      "       -1.65580118e-02])\n",
      "INFO:root:minimize results at train index = 7 = \n",
      "      fun: -511.19899137861967\n",
      " hess_inv: <5x5 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([ 6.70354439e-01,  7.04358172e-01,  3.62172159e-01,  5.02182047e+00,\n",
      "       -1.33750115e+03])\n",
      "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 414\n",
      "      nit: 21\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 7.03512992e+01,  2.51501045e+02,  1.97538475e+02,  1.07696038e+01,\n",
      "       -1.54404686e-02])\n",
      "INFO:root:minimize results at train index = 8 = \n",
      "      fun: nan\n",
      " hess_inv: <5x5 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([nan, nan, nan, nan, nan])\n",
      "  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
      "     nfev: 6\n",
      "      nit: 0\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 3.04570209,  2.24217514, -0.08133855,  1.28213263,  2.05664237])\n",
      "INFO:root:minimize results at train index = 9 = \n",
      "      fun: -498.02310754697055\n",
      " hess_inv: <5x5 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([ 5.67922598e-02, -4.01314537e-02, -3.51906237e-01,  6.67120048e+00,\n",
      "        8.82966128e+02])\n",
      "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 306\n",
      "      nit: 18\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 9.53665443e+01,  4.45475814e+01,  7.43283025e+01,  5.98366039e+00,\n",
      "       -1.12601205e-02])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      fun: -498.02310754697055\n",
       " hess_inv: <5x5 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([ 5.67922598e-02, -4.01314537e-02, -3.51906237e-01,  6.67120048e+00,\n",
       "        8.82966128e+02])\n",
       "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "     nfev: 306\n",
       "      nit: 18\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([ 9.53665443e+01,  4.45475814e+01,  7.43283025e+01,  5.98366039e+00,\n",
       "       -1.12601205e-02])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 1: with Kernal in [2] eqn(5), gaussian with precision\n",
    "min_obj_fn_1 = np.Inf\n",
    "theta_min_1 = None\n",
    "res_min_1 = None\n",
    "logger = logging.getLogger()\n",
    "for i in np.arange(n_trials):\n",
    "    theta_0 = np.random.normal(loc=2,scale = 1,size=D_x+2)\n",
    "    res = minimize(fun=neg_ll,x0=theta_0,args=(X[:N_t],t_prime[:N_t],kernel_mtx_gaussian_w_precision_fn)\\\n",
    "                             ,method='L-BFGS-B')\n",
    "    logger.info(f'minimize results at train index = {i} = \\n{res}')\n",
    "    if res['fun'] < min_obj_fn_1:\n",
    "        res_min_1= res\n",
    "        min_obj_fn_1 = res['fun']\n",
    "        theta_min_1 = res['x']\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:minimize results at train index = 0 = \n",
      "      fun: -364.07006704236153\n",
      " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([0.03140599, 0.13594672])\n",
      "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 78\n",
      "      nit: 12\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([3.32188542, 1.91267712])\n",
      "INFO:root:minimize results at train index = 1 = \n",
      "      fun: -364.0713846408736\n",
      " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([ 0.74581408, -0.16929107])\n",
      "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 66\n",
      "      nit: 10\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([3.32699845, 1.92231163])\n",
      "/home/mbaddar/mbaddar/my_projects/phd/implementation/gprl/gprl_venv/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  import sys\n",
      "/home/mbaddar/mbaddar/my_projects/phd/implementation/gprl/gprl_venv/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in log\n",
      "  import sys\n",
      "INFO:root:minimize results at train index = 2 = \n",
      "      fun: nan\n",
      " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([nan, nan])\n",
      "  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
      "     nfev: 42\n",
      "      nit: 2\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 919547.85757578, -508859.0353374 ])\n",
      "INFO:root:minimize results at train index = 3 = \n",
      "      fun: -364.0713733575843\n",
      " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([-0.00680984,  0.64024448])\n",
      "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 45\n",
      "      nit: 5\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([3.32492739, 1.9192337 ])\n",
      "INFO:root:minimize results at train index = 4 = \n",
      "      fun: -364.0714947363265\n",
      " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([-0.10971917, -0.08286634])\n",
      "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 153\n",
      "      nit: 14\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([3.32598079, 1.92150862])\n",
      "INFO:root:minimize results at train index = 5 = \n",
      "      fun: -364.0634800082743\n",
      " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([-1.21255539,  0.76429387])\n",
      "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 60\n",
      "      nit: 7\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([3.33379126, 1.94435711])\n",
      "INFO:root:minimize results at train index = 6 = \n",
      "      fun: -364.07147312480697\n",
      " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([ 0.27663418, -0.18475248])\n",
      "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 69\n",
      "      nit: 10\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([3.32658572, 1.92218588])\n",
      "INFO:root:minimize results at train index = 7 = \n",
      "      fun: -364.0714462080646\n",
      " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([0.22094468, 0.47249387])\n",
      "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 102\n",
      "      nit: 14\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([3.32623542, 1.92307786])\n",
      "INFO:root:minimize results at train index = 8 = \n",
      "      fun: -364.0714968458232\n",
      " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([-0.26383873, -0.37877612])\n",
      "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 51\n",
      "      nit: 11\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([3.3263192 , 1.92236739])\n",
      "INFO:root:minimize results at train index = 9 = \n",
      "      fun: -364.0686501336685\n",
      " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
      "      jac: array([ 1.36233211, -0.22944846])\n",
      "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "     nfev: 63\n",
      "      nit: 7\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([3.32465381, 1.91227627])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      fun: -364.0686501336685\n",
       " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>\n",
       "      jac: array([ 1.36233211, -0.22944846])\n",
       "  message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "     nfev: 63\n",
       "      nit: 7\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([3.32465381, 1.91227627])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Method 2 , ordinary Gaussian Kernel\n",
    "n_trials = 10\n",
    "min_obj_fn_2 = np.Inf\n",
    "theta_min_2 = None\n",
    "res_min_2 = None\n",
    "logger = logging.getLogger()\n",
    "for i in np.arange(n_trials):\n",
    "    theta_0 = np.random.normal(loc=2,scale = 1,size=2)\n",
    "    res = minimize(fun=neg_ll,x0=theta_0,args=(X[:N_t],t_prime[:N_t],kernel_gaussian_fn)\\\n",
    "                             ,method='L-BFGS-B')\n",
    "    logger.info(f'minimize results at train index = {i} = \\n{res}')\n",
    "    if res['fun'] < min_obj_fn_2:\n",
    "        res_min_2 = res\n",
    "        min_obj_fn_2 = res['fun']\n",
    "        theta_min_2 = res['x']\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in-sample prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_pred(x,X,t,kernel_fn,theta):\n",
    "    k = kernel_fn(theta,x,X)\n",
    "    K = kernel_fn(theta,X,X)\n",
    "    y_mean = np.matmul(k,np.matmul(inv(K),t))\n",
    "    return y_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_idx = np.arange(N_t,(N_t+100))\n",
    "x = X[sample_idx,:]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_prime[sample_idx,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ = y_pred(x,X[:N_t],t_prime[:N_t],kernel_mtx_gaussian_w_precision_fn,theta_min_1)\n",
    "y_pred_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007448532246993616"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_y1 = sqrt(mean_squared_error(y_true = t_prime[sample_idx,0],y_pred=y_pred_[:,0]))\n",
    "rmse_y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02007143502506732"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_y2 = sqrt(mean_squared_error(y_true = t_prime[sample_idx,1],y_pred=y_pred_[:,1]))\n",
    "rmse_y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to verify , plot predictive distribution for values from kernel 1 and kernel 2 , should be the same !\n",
    "# is it called similarity verification !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ref:\n",
    "1. http://krasserm.github.io/2018/03/19/gaussian-processes/\n",
    "2. https://papers.nips.cc/paper/2420-gaussian-processes-in-reinforcement-learning.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "1. Deep understanding for optimization methods in <br>\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
