{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "1. develop kernel \n",
    "2. validate kernel with h-param variation as in ref#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import pearsonr\n",
    "from numpy.linalg import cholesky, det, lstsq,inv\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(xp,xq,p,q,lambda_diag,v,sigma_n):\n",
    "    logger = logging.getLogger()\n",
    "    \"\"\"\n",
    "    xp: float array mx1 vector of input point p\n",
    "    xq : float array\n",
    "    lambda_ 1Xm represents the diagonal of the hyperparemter diagonal matrix lambda_\n",
    "    v: hyperparemter\n",
    "    sigma2_n : hyperparameter\n",
    "    \"\"\"\n",
    "    #TODO check all inputes are doubles\n",
    "    lambda_power_minus_1 = np.diag(np.reciprocal(lambda_diag))\n",
    "    exponent_ = np.matmul(np.matmul(-(xp-xq).T,lambda_power_minus_1),(xp-xq))\n",
    "    delta_pq = 1.0 if p==q else 0.0\n",
    "    kernel = v**2 * np.exp(-0.5*exponent)+delta_pq*sigma_n**2\n",
    "    #logging\n",
    "    logger.debug(f'lambda diag= {lambda_diag}')\n",
    "    logger.debug(f'lambda_pow_minus_1 {lambda_power_minus_1}')\n",
    "    logger.debug(f'term1 = -(xp-xq).T * lambda_pow_minus_1 = {term1}')\n",
    "    logger.debug(f'exponent = {exponent_}')\n",
    "    ######\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't delete\n",
    "xp = pd.Series([1.0,13.0]).values.reshape(-1,1)\n",
    "xq = pd.Series([1.0,2.0]).values.reshape(-1,1)\n",
    "lambda_diag = [0.2,0.3]\n",
    "lambda_power_minus_1 = np.diag(np.reciprocal(lambda_diag))\n",
    "lambda_power_minus_1\n",
    "(xp-xq).T.shape\n",
    "term1 = np.matmul(-(xp-xq).T,lambda_power_minus_1)\n",
    "exponent = np.matmul(term1,(xp-xq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.83217475e-176]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k= kernel(xp=xp,xq=xq,p=1,q=2,lambda_diag=lambda_diag,sigma_n=0.05,v=1.0)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9999074085159615, 0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick validation :kernel vs euclidean distance , comment out , time consuming\n",
    "# dim = 100,5\n",
    "# X = np.random.normal(loc=0,scale=1.0,size=dim)\n",
    "# k = np.zeros(dim[0]**2)\n",
    "# euc = np.zeros(dim[0]**2)\n",
    "# idx = 0\n",
    "# for i in np.arange(dim[0]):\n",
    "#     for j in np.arange(dim[0]):\n",
    "#         xp = X[i,].reshape(-1,1)\n",
    "#         xq = X[j,].reshape(-1,1)\n",
    "#         k[idx] = kernel(xp=xp,xq=xq,p=i,q=j,lambda_diag=[0.2,0.1,0.3,0.4,0.5],sigma_n=0.05,v=1.0)\n",
    "#         euc[idx] = euclidean(u=xp,v=xq)\n",
    "#         idx = idx + 1\n",
    "# pearsonr(x=k,y=np.reciprocal(euc+0.01))\n",
    "# if corr is +ve and significant, then we are good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test scipy minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "def fn(x,x0,c):\n",
    "    return np.sum((x-x0)**2)+c\n",
    "x0 = np.array([[1,1]])\n",
    "x = np.array([[2,2]])\n",
    "\n",
    "x_min = minimize(fun=fn,x0=np.array([[0,0]]),args=(x0,10))\n",
    "x_min.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def kernel as matrix operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Note: in http://krasserm.github.io/2018/03/19/gaussian-processes/\n",
    "# in the implementatoin of Gaussian kernel , the sum of the two squared terms is broadcast sum\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.add.html\n",
    "# https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\n",
    "# why ? as in first term , reshape(-1,1) is used , then the row sum shape is converted from (m,) to (m,1)\n",
    "# then it is summed with array of shape (n,) then shapes are not equal, even if m==n , then the second dimensions \n",
    "# are not equal\n",
    "import numpy as np\n",
    "def kernel(X1, X2, l=1.0, sigma_f=1.0):\n",
    "    ''' Isotropic squared exponential kernel. Computes a covariance matrix from points in X1 and X2. Args: X1: Array of m points (m x d). X2: Array of n points (n x d). Returns: Covariance matrix (m x n). '''\n",
    "    # the sume is broad cast \n",
    "    sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist)\n",
    "\n",
    "X1 = np.random.normal(loc = 10,scale = 2, size = (10,2))\n",
    "X2 = np.random.normal(loc = 10,scale = 2, size = (10,2))\n",
    "X1_sum= np.sum(X1**2, 1).reshape(-1,1)\n",
    "X2_sum= np.sum(X2**2, 1)\n",
    "X2_sum_2= np.sum(X2**2, 1).reshape(-1,1)\n",
    "tot = X1_sum + X2_sum\n",
    "print((X1_sum + X2_sum).shape)\n",
    "print((X1_sum + X2_sum_2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0025, 1.0025, 1.0025, 1.0025, 1.0025, 1.0025, 1.0025, 1.0025,\n",
       "       1.0025, 1.0025])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def kernel_mtx(X1,X2,lambda_diag,v,sigma_n):\n",
    "    ## from http://krasserm.github.io/2018/03/19/gaussian-processes/\n",
    "    ## sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(level=logging.INFO)\n",
    "    assert X1.shape[1] == X2.shape[1]\n",
    "    assert X1.shape[1] == len(lambda_diag)\n",
    "    \n",
    "    \n",
    "    lambda_power_minus_1 = np.diag(np.reciprocal(lambda_diag))\n",
    "    # quadratic terms are scale by inverse precision (variance) , precision = lambda (Captial case)\n",
    "    X1_sq_sum_scaled = np.sum(a=np.matmul(X1**2,lambda_power_minus_1),axis=1)\n",
    "    X2_sq_sum_scale = np.sum(a=np.matmul(X2**2,lambda_power_minus_1),axis=1)\n",
    "    X1_matmul_X2_T_scaled = np.matmul(np.matmul(X1,lambda_power_minus_1),X2.T)\n",
    "    \n",
    "    sqdist_scaled = X1_sq_sum_scaled.reshape(-1,1)+X2_sq_sum_scale-2*X1_matmul_X2_T_scaled\n",
    "    sigma_n_sq = np.zeros((X1.shape[0],X2.shape[0]))\n",
    "    if X1.shape[0] == X2.shape[0]: #same data-set , or at least comparable\n",
    "        sigma_n_sq = np.diag(np.repeat(sigma_n**2,X1.shape[0]))\n",
    "        \n",
    "    kernel = v**2 * np.exp(-0.5*sqdist_scaled)+sigma_n_sq\n",
    "    logger.debug(f'sqdist_scaled = {sqdist_scaled}')\n",
    "    return kernel\n",
    "\n",
    "\n",
    "M = 10\n",
    "N = 8\n",
    "D = 3\n",
    "X1 = np.random.normal(loc = 10, scale  = 2, size = (M,D))\n",
    "X2 = np.random.normal(loc = 5, scale  = 1, size = (N,D))\n",
    "lambda_diag = np.random.normal(loc = 1, scale = 0.01, size = D)\n",
    "v = 1\n",
    "sigma_n = 0.05\n",
    "X1.shape\n",
    "k = kernel_mtx(X1,X1,lambda_diag,v,sigma_n)\n",
    "np.diag(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_fn(theta,X1,X2):\n",
    "    assert X1.shape[1] == X2.shape[1]\n",
    "    D = X1.shape[1]\n",
    "    lambda_diag=theta[0:D]\n",
    "    v= theta[D]\n",
    "    sigma_n = theta[D+1]\n",
    "    return kernel_mtx(X1,X1,lambda_diag,v,sigma_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118.31066424897398"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def neg_ll(theta,X,t,kernel_fn):\n",
    "    assert X.shape[0]==t.shape[0]\n",
    "    K = kernel_fn(theta,X,X)\n",
    "    #Y_train.T.dot(inv(K).dot(Y_train)\n",
    "    neg_ll = np.log(det(K))+t.T.dot(inv(K).dot(t))\n",
    "    return neg_ll[0][0]\n",
    "t = np.random.normal(loc=5,scale = 1,size = (M,1))\n",
    "neg_ll(X=X1,t=t,kernel_fn=kernel_fn,theta=theta_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 20.792175637627842\n",
       " hess_inv: array([[ 6.97177912e+06,  0.00000000e+00,  1.27686011e+07,\n",
       "         1.88385123e+03, -1.23102409e+02],\n",
       "       [ 0.00000000e+00,  4.76837158e-07,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00],\n",
       "       [ 1.27686011e+07,  0.00000000e+00,  2.33853591e+07,\n",
       "         3.46226456e+03, -2.25456295e+02],\n",
       "       [ 1.88385123e+03,  0.00000000e+00,  3.46226456e+03,\n",
       "         3.38390309e+00, -1.82766046e-02],\n",
       "       [-1.23102409e+02,  0.00000000e+00, -2.25456295e+02,\n",
       "        -1.82766046e-02,  2.65617337e-02]])\n",
       "      jac: array([-3.86238098e-05, -4.76837158e-06,  2.62260437e-05, -1.43861771e-03,\n",
       "        1.07836723e-02])\n",
       "  message: 'Desired error not necessarily achieved due to precision loss.'\n",
       "     nfev: 673\n",
       "      nit: 39\n",
       "     njev: 96\n",
       "   status: 2\n",
       "  success: False\n",
       "        x: array([1.25163671e+03, 1.58452741e+03, 2.29342511e+03, 5.07289990e+00,\n",
       "       1.26582348e+00])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(fun=neg_ll,x0=theta_0,args=(X1,t,kernel_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-3248ab5a93be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'x_dot'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mD_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtheta_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mD_x\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_prime'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'x_dot_prime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneg_ll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtheta_0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('../data/train_data.csv')\n",
    "train_data.head()\n",
    "X = train_data[['x','x_dot','F']]\n",
    "D_x = X.values.shape[1]\n",
    "theta_0 = np.random(loc=2,scale = 1,size=D_x+2)\n",
    "t = train_data[['x_prime','x_dot_prime']]\n",
    "minimize(fun=neg_ll,x0=theta_0,args=(X.values,t.values,kernel_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ref:\n",
    "1. http://krasserm.github.io/2018/03/19/gaussian-processes/\n",
    "2. https://papers.nips.cc/paper/2420-gaussian-processes-in-reinforcement-learning.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
